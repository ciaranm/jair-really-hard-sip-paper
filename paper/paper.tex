% vim: set spell spelllang=en tw=100 :

\documentclass[twoside,11pt]{article}
\usepackage{jair}
\usepackage{theapa}
\usepackage{rawfonts}
\usepackage{complexity}
\usepackage{cleveref}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{nicefrac}
\usepackage{csquotes}

% \usepackage{showframe}

\usetikzlibrary{decorations, decorations.pathreplacing, calc, backgrounds}

\jairheading{1}{2016}{0-0}{0/0}{0/0}
\ShortHeadings{Why Real-World Subgraph Matching is Easy}
{McCreesh, Prosser \& Trimble}
\firstpageno{1}

\newcommand{\citet}[1]{\citeA{#1}}
\newcommand{\citep}[1]{\cite{#1}}

% http://tex.stackexchange.com/questions/22100/the-bar-and-overline-commands
\newcommand{\shortoverline}[1]{\mkern 1.5mu\overline{\mkern-1.5mu#1\mkern-1.5mu}\mkern 1.5mu}

\definecolor{uofgsandstone}{rgb}{0.321569, 0.278431, 0.231373}
\definecolor{uofglawn}{rgb}{0.517647, 0.741176, 0}
\definecolor{uofgcobalt}{rgb}{0, 0.615686, 0.92549}
\definecolor{uofgpumpkin}{rgb}{1.0, 0.72549, 0.282353}
\definecolor{uofgthistle}{rgb}{0.584314, 0.070588, 0.447059}

\begin{document}

\title{Why Real-World Subgraph Matching is Easy, and \\ What This Means for Filter / Verify in
Graph Databases \thanks{Part of this paper appeared at IJCAI 2016
\citep{DBLP:conf/ijcai/McCreeshPT16}; \cref{section:introduction} has been expanded and
\cref{section:models,section:labelled,section:filterverify} are new material.}}

\author{\name Ciaran McCreesh \thanks{This work was supported by the Engineering and Physical Sciences
            Research Council [grant number EP/K503058/1]} \email c.mccreesh.1@research.gla.ac.uk \\
       \name Patrick Prosser \email patrick.prosser@glasgow.ac.uk \\
       \name James Trimble \\
   \addr University of Glasgow, Glasgow, Scotland}
\maketitle

\begin{abstract}
    Despite subgraph isomorphism being \NP-complete, modern algorithms can operate comfortably on
    real-world graphs with thousands of vertices. However, they cannot handle arbitrary instances of
    this size. We show how to generate ``really hard'' random instances for subgraph isomorphism
    problems, which are computationally challenging with only a couple of hundred vertices. For the
    non-induced version of the problem, these instances lie on a satisfiable / unsatisfiable phase
    transition, whose location we can predict; for the induced variant, much richer behaviour is
    observed, and constrainedness gives a better measure of difficulty than does proximity to a
    phase transition. In both cases, we can use this to understand and improve variable and value
    ordering heuristics.

    When labels are introduced, as commonly seen in graph databases, additional rich behaviour
    arises.  We show that when only slightly better algorithms are used, all commonly used benchmark
    suites for evaluating graph database systems become trivial, and that existing notions of
    difficulty are based almost entirely upon unnecessarily poor performance from overly simplistic
    subgraph isomorphism algorithms. With this in mind, we argue that the widely researched ``filter
    / verify'' indexing technique used in graph databases is a flawed design based upon a repeated
    misunderstanding of what makes subgraph isomorphism hard. We explain why this technique would
    not be beneficial, even with harder instances than those currently used, when paired with any
    reasonable modern subgraph isomorphism algorithm.
\end{abstract}

\section{Introduction}\label{section:introduction}

The \emph{non-induced subgraph isomorphism problem} is to find an injective mapping from a given
pattern graph to a given target graph which preserves adjacency---in essence, we are ``finding a
copy of'' the pattern inside the target. The \emph{induced} variant of the problem additionally
requires that the mapping preserve non-adjacency, so there are no ``extra edges'' in the copy of the
pattern that we find. We illustrate both variants in \cref{figure:sip}.  Although these problems
are \NP-complete, modern practical subgraph isomorphism algorithms can handle problem instances
with many hundreds of vertices in the pattern graph, and up to ten thousand vertices in the target
graph \cite{Cordella:2004,Solnon:2010,Audemard:2014,McCreesh:2015}, leading to successful
application in areas such as computer vision \cite{Damiand:2011,Solnon:2015}, biochemistry
\cite{Giugno:2013,Carletti:2015}, and pattern recognition \citep{Conte:2004}.  In particular,
a labelled version of subgraph isomorphism is one of the key components in supporting complex
queries in graph databases---typically, these systems store a fixed set of target graphs, and for a
sequence of pattern queries, they must return every target graph which contains that pattern.  One
common approach to this problem is to combine a subgraph isomorphism algorithm with an indexing
system in a so-called ``filter / verify'' model, where invariants are used to attempt to pre-exclude
unsatisfiable instances to avoid the cost of a subgraph isomorphism call. In this context, the terms
\emph{matching} and \emph{verification} are often used for the subgraph isomorphism step (or a
slightly broader problem, for example permitting wildcards).

\begin{figure}[t]
    \centering
    \begin{tikzpicture}[scale=0.5,every node/.style={font=\footnotesize}]
        \begin{scope}[yshift=-2cm]
            \node [draw, circle, fill=uofgcobalt, inner sep=1.5pt] (Na) at (1,  0) {1};
            \node [draw, circle, fill=uofgpumpkin, inner sep=1.5pt] (Nb) at (1, -1.5) {2};
            \node [draw, circle, fill=uofglawn, inner sep=1.5pt] (Nc) at (0, -3) {3};
            \node [draw, circle, fill=uofgthistle, inner sep=1.5pt] (Nd) at (2, -3) {4};

            \draw [ultra thick] (Na) -- (Nb);
            \draw [ultra thick] (Nb) -- (Nc);
            \draw [ultra thick] (Nc) -- (Nd);
            \draw [ultra thick] (Nb) -- (Nd);

            \node [draw, circle, fill=uofgcobalt, inner sep=1.5pt] (N1) at (5.5,  0) {1};
            \node [draw, circle, fill=white, inner sep=1.5pt] (N2) at (7.5,  0) {2};
            \node [draw, circle, fill=uofgpumpkin, inner sep=1.5pt] (N3) at (5.5, -1.5) {3};
            \node [draw, circle, fill=white, inner sep=1.5pt] (N4) at (7.5, -1.5) {4};
            \node [draw, circle, fill=uofglawn, inner sep=1.5pt] (N5) at (5.5, -3) {5};
            \node [draw, circle, fill=uofgthistle, inner sep=1.5pt] (N6) at (7.5, -3) {6};

            \draw [thick, color=uofgsandstone!50] (N1) -- (N2);
            \draw [ultra thick] (N1) -- (N3);
            \draw [thick, color=uofgsandstone!50] (N1) -- (N4);
            \draw [thick, color=uofgsandstone!50] (N2) -- (N4);
            \draw [ultra thick] (N3) -- (N5);
            \draw [ultra thick] (N3) -- (N6);
            \draw [thick, color=uofgsandstone!50] (N4) -- (N6);
            \draw [ultra thick] (N5) -- (N6);
            \draw [thick, color=uofgsandstone!50] (N2) to [in=45, out=315] (N6);
            \draw [thick, color=uofgsandstone!50] (N1) to [in=135, out=225] (N5);

            \node [draw, circle, fill=white, inner sep=1.5pt] (M1) at (-5.5,  0) {1};
            \node [draw, circle, fill=uofgcobalt, inner sep=1.5pt] (M2) at (-3.5,  0) {2};
            \node [draw, circle, fill=uofgthistle, inner sep=1.5pt] (M3) at (-5.5, -1.5) {3};
            \node [draw, circle, fill=white, inner sep=1.5pt] (M4) at (-3.5, -1.5) {4};
            \node [draw, circle, fill=uofglawn, inner sep=1.5pt] (M5) at (-5.5, -3) {5};
            \node [draw, circle, fill=uofgpumpkin, inner sep=1.5pt] (M6) at (-3.5, -3) {6};

            \draw [thick, color=uofgsandstone!50] (M1) -- (M2);
            \draw [thick, color=uofgsandstone!50] (M1) -- (M3);
            \draw [thick, color=uofgsandstone!50] (M1) -- (M4);
            \draw [thick, color=uofgsandstone!50] (M2) -- (M4);
            \draw [ultra thick] (M3) -- (M5);
            \draw [ultra thick] (M3) -- (M6);
            \draw [thick, color=uofgsandstone!50] (M4) -- (M6);
            \draw [ultra thick] (M5) -- (M6);
            \draw [ultra thick] (M2) to [in=45, out=315] (M6);
            \draw [thick, color=uofgsandstone!50] (M1) to [in=135, out=225] (M5);

            \node [anchor=center, font=\Large] (A1) at (-1.5, -1.5) { $\hookleftarrow$ };
            \node [anchor=center, font=\Large] (A2) at ( 3.5, -1.5) { $\rightarrowtail$ };
        \end{scope}
        \begin{scope}[xshift=13cm]
            \foreach \n in {1, ..., 6}{
                \node [draw, circle, fill=uofgcobalt, inner sep=1pt] (N1\n) at (\n, 0) {\n};
            }

            \draw [decorate, decoration={brace, raise=0.2cm}, very thick] (N11.north west) -- (N16.north
                east) node [midway, above=0.3cm] { $1 \mapsto$ };

            \foreach \n in {1, ..., 6}{
                \node [draw, circle, fill=uofgpumpkin, inner sep=1pt] (N2\n) at (0, -\n) {\n};
            }

            \draw [decorate, decoration={brace, raise=0.2cm, mirror}, very thick] (N21.north west) --
                (N26.south west) node [midway, left=0.3cm] { $2 \mapsto$ };

            \foreach \n in {1, ..., 6}{
                \node [draw, circle, fill=uofglawn, inner sep=1pt] (N3\n) at (7, -\n) {\n};
            }

            \draw [decorate, decoration={brace, raise=0.2cm}, very thick] (N31.north east) -- (N36.south
                east) node [midway, right=0.3cm] { $3 \mapsto$ };

            \foreach \n in {1, ..., 6}{
                \node [draw, circle, fill=uofgthistle, inner sep=1pt] (N4\n) at (\n, -7) {\n};
            }

            \draw [decorate, decoration={brace, raise=0.2cm, mirror}, very thick] (N41.south west) --
                (N46.south east) node [midway, below=0.3cm] { $4 \mapsto$ };

            \begin{scope}[on background layer]
                \draw [color=uofgsandstone!50] ($(N11)!0.8!(N11.south)$) -- ($(N22)!0.8!(N22.east)$);
                \draw [color=uofgsandstone!50] ($(N11)!0.8!(N11.south)$) -- ($(N23)!0.8!(N23.east)$);
                \draw [color=uofgsandstone!50] ($(N11)!0.8!(N11.south)$) -- ($(N24)!0.8!(N24.east)$);
                \draw [color=uofgsandstone!50] ($(N11)!0.8!(N11.south)$) -- ($(N25)!0.8!(N25.east)$);
                \draw [color=uofgsandstone!50] ($(N11)!0.8!(N11.south)$) -- ($(N36)!0.8!(N36.west)$);
                \draw [color=uofgsandstone!50] ($(N11)!0.8!(N11.south)$) -- ($(N46)!0.8!(N46.north)$);
                \draw [color=uofgsandstone!50] ($(N21)!0.8!(N21.east)$) -- ($(N12)!0.8!(N12.south)$);
                \draw [color=uofgsandstone!50] ($(N21)!0.8!(N21.east)$) -- ($(N32)!0.8!(N32.west)$);
                \draw [color=uofgsandstone!50] ($(N21)!0.8!(N21.east)$) -- ($(N42)!0.8!(N42.north)$);
                \draw [color=uofgsandstone!50] ($(N21)!0.8!(N21.east)$) -- ($(N13)!0.8!(N13.south)$);
                \draw [color=uofgsandstone!50] ($(N21)!0.8!(N21.east)$) -- ($(N33)!0.8!(N33.west)$);
                \draw [color=uofgsandstone!50] ($(N21)!0.8!(N21.east)$) -- ($(N43)!0.8!(N43.north)$);
                \draw [color=uofgsandstone!50] ($(N21)!0.8!(N21.east)$) -- ($(N14)!0.8!(N14.south)$);
                \draw [color=uofgsandstone!50] ($(N21)!0.8!(N21.east)$) -- ($(N34)!0.8!(N34.west)$);
                \draw [color=uofgsandstone!50] ($(N21)!0.8!(N21.east)$) -- ($(N44)!0.8!(N44.north)$);
                \draw [color=uofgsandstone!50] ($(N21)!0.8!(N21.east)$) -- ($(N15)!0.8!(N15.south)$);
                \draw [color=uofgsandstone!50] ($(N21)!0.8!(N21.east)$) -- ($(N35)!0.8!(N35.west)$);
                \draw [color=uofgsandstone!50] ($(N21)!0.8!(N21.east)$) -- ($(N45)!0.8!(N45.north)$);
                \draw [color=uofgsandstone!50] ($(N31)!0.8!(N31.west)$) -- ($(N22)!0.8!(N22.east)$);
                \draw [color=uofgsandstone!50] ($(N31)!0.8!(N31.west)$) -- ($(N42)!0.8!(N42.north)$);
                \draw [color=uofgsandstone!50] ($(N31)!0.8!(N31.west)$) -- ($(N23)!0.8!(N23.east)$);
                \draw [color=uofgsandstone!50] ($(N31)!0.8!(N31.west)$) -- ($(N43)!0.8!(N43.north)$);
                \draw [color=uofgsandstone!50] ($(N31)!0.8!(N31.west)$) -- ($(N24)!0.8!(N24.east)$);
                \draw [color=uofgsandstone!50] ($(N31)!0.8!(N31.west)$) -- ($(N44)!0.8!(N44.north)$);
                \draw [color=uofgsandstone!50] ($(N31)!0.8!(N31.west)$) -- ($(N25)!0.8!(N25.east)$);
                \draw [color=uofgsandstone!50] ($(N31)!0.8!(N31.west)$) -- ($(N45)!0.8!(N45.north)$);
                \draw [color=uofgsandstone!50] ($(N31)!0.8!(N31.west)$) -- ($(N16)!0.8!(N16.south)$);
                \draw [color=uofgsandstone!50] ($(N41)!0.8!(N41.north)$) -- ($(N22)!0.8!(N22.east)$);
                \draw [color=uofgsandstone!50] ($(N41)!0.8!(N41.north)$) -- ($(N32)!0.8!(N32.west)$);
                \draw [color=uofgsandstone!50] ($(N41)!0.8!(N41.north)$) -- ($(N23)!0.8!(N23.east)$);
                \draw [color=uofgsandstone!50] ($(N41)!0.8!(N41.north)$) -- ($(N33)!0.8!(N33.west)$);
                \draw [color=uofgsandstone!50] ($(N41)!0.8!(N41.north)$) -- ($(N24)!0.8!(N24.east)$);
                \draw [color=uofgsandstone!50] ($(N41)!0.8!(N41.north)$) -- ($(N34)!0.8!(N34.west)$);
                \draw [color=uofgsandstone!50] ($(N41)!0.8!(N41.north)$) -- ($(N25)!0.8!(N25.east)$);
                \draw [color=uofgsandstone!50] ($(N41)!0.8!(N41.north)$) -- ($(N35)!0.8!(N35.west)$);
                \draw [color=uofgsandstone!50] ($(N41)!0.8!(N41.north)$) -- ($(N16)!0.8!(N16.south)$);
                \draw [color=uofgsandstone!50] ($(N12)!0.8!(N12.south)$) -- ($(N33)!0.8!(N33.west)$);
                \draw [color=uofgsandstone!50] ($(N12)!0.8!(N12.south)$) -- ($(N24)!0.8!(N24.east)$);
                \draw [color=uofgsandstone!50] ($(N12)!0.8!(N12.south)$) -- ($(N45)!0.8!(N45.north)$);
                \draw [color=uofgsandstone!50] ($(N22)!0.8!(N22.east)$) -- ($(N14)!0.8!(N14.south)$);
                \draw [color=uofgsandstone!50] ($(N22)!0.8!(N22.east)$) -- ($(N34)!0.8!(N34.west)$);
                \draw [color=uofgsandstone!50] ($(N22)!0.8!(N22.east)$) -- ($(N44)!0.8!(N44.north)$);
                \draw [color=uofgsandstone!50] ($(N22)!0.8!(N22.east)$) -- ($(N16)!0.8!(N16.south)$);
                \draw [color=uofgsandstone!50] ($(N22)!0.8!(N22.east)$) -- ($(N36)!0.8!(N36.west)$);
                \draw [color=uofgsandstone!50] ($(N22)!0.8!(N22.east)$) -- ($(N46)!0.8!(N46.north)$);
                \draw [color=uofgsandstone!50] ($(N32)!0.8!(N32.west)$) -- ($(N13)!0.8!(N13.south)$);
                \draw [color=uofgsandstone!50] ($(N32)!0.8!(N32.west)$) -- ($(N24)!0.8!(N24.east)$);
                \draw [color=uofgsandstone!50] ($(N32)!0.8!(N32.west)$) -- ($(N44)!0.8!(N44.north)$);
                \draw [color=uofgsandstone!50] ($(N32)!0.8!(N32.west)$) -- ($(N15)!0.8!(N15.south)$);
                \draw [color=uofgsandstone!50] ($(N32)!0.8!(N32.west)$) -- ($(N26)!0.8!(N26.east)$);
                \draw [color=uofgsandstone!50] ($(N32)!0.8!(N32.west)$) -- ($(N46)!0.8!(N46.north)$);
                \draw [color=uofgsandstone!50] ($(N42)!0.8!(N42.north)$) -- ($(N13)!0.8!(N13.south)$);
                \draw [color=uofgsandstone!50] ($(N42)!0.8!(N42.north)$) -- ($(N24)!0.8!(N24.east)$);
                \draw [color=uofgsandstone!50] ($(N42)!0.8!(N42.north)$) -- ($(N34)!0.8!(N34.west)$);
                \draw [color=uofgsandstone!50] ($(N42)!0.8!(N42.north)$) -- ($(N15)!0.8!(N15.south)$);
                \draw [color=uofgsandstone!50] ($(N42)!0.8!(N42.north)$) -- ($(N26)!0.8!(N26.east)$);
                \draw [color=uofgsandstone!50] ($(N42)!0.8!(N42.north)$) -- ($(N36)!0.8!(N36.west)$);
                \draw [color=uofgsandstone!50] ($(N13)!0.8!(N13.south)$) -- ($(N34)!0.8!(N34.west)$);
                \draw [color=uofgsandstone!50] ($(N13)!0.8!(N13.south)$) -- ($(N44)!0.8!(N44.north)$);
                \draw [color=uofgsandstone!50] ($(N13)!0.8!(N13.south)$) -- ($(N25)!0.8!(N25.east)$);
                \draw [color=uofgsandstone!50] ($(N13)!0.8!(N13.south)$) -- ($(N26)!0.8!(N26.east)$);
                \draw [color=uofgsandstone!50] ($(N23)!0.8!(N23.east)$) -- ($(N15)!0.8!(N15.south)$);
                \draw [color=uofgsandstone!50] ($(N23)!0.8!(N23.east)$) -- ($(N35)!0.8!(N35.west)$);
                \draw [color=uofgsandstone!50] ($(N23)!0.8!(N23.east)$) -- ($(N45)!0.8!(N45.north)$);
                \draw [color=uofgsandstone!50] ($(N23)!0.8!(N23.east)$) -- ($(N16)!0.8!(N16.south)$);
                \draw [color=uofgsandstone!50] ($(N23)!0.8!(N23.east)$) -- ($(N36)!0.8!(N36.west)$);
                \draw [color=uofgsandstone!50] ($(N23)!0.8!(N23.east)$) -- ($(N46)!0.8!(N46.north)$);
                \draw [color=uofgsandstone!50] ($(N33)!0.8!(N33.west)$) -- ($(N14)!0.8!(N14.south)$);
                \draw [color=uofgsandstone!50] ($(N33)!0.8!(N33.west)$) -- ($(N25)!0.8!(N25.east)$);
                \draw [color=uofgsandstone!50] ($(N33)!0.8!(N33.west)$) -- ($(N45)!0.8!(N45.north)$);
                \draw [color=uofgsandstone!50] ($(N33)!0.8!(N33.west)$) -- ($(N26)!0.8!(N26.east)$);
                \draw [color=uofgsandstone!50] ($(N33)!0.8!(N33.west)$) -- ($(N46)!0.8!(N46.north)$);
                \draw [color=uofgsandstone!50] ($(N43)!0.8!(N43.north)$) -- ($(N14)!0.8!(N14.south)$);
                \draw [color=uofgsandstone!50] ($(N43)!0.8!(N43.north)$) -- ($(N25)!0.8!(N25.east)$);
                \draw [color=uofgsandstone!50] ($(N43)!0.8!(N43.north)$) -- ($(N36)!0.8!(N36.west)$);
                \draw [color=uofgsandstone!50] ($(N14)!0.8!(N14.south)$) -- ($(N35)!0.8!(N35.west)$);
                \draw [color=uofgsandstone!50] ($(N14)!0.8!(N14.south)$) -- ($(N45)!0.8!(N45.north)$);
                \draw [color=uofgsandstone!50] ($(N14)!0.8!(N14.south)$) -- ($(N26)!0.8!(N26.east)$);
                \draw [color=uofgsandstone!50] ($(N24)!0.8!(N24.east)$) -- ($(N16)!0.8!(N16.south)$);
                \draw [color=uofgsandstone!50] ($(N24)!0.8!(N24.east)$) -- ($(N36)!0.8!(N36.west)$);
                \draw [color=uofgsandstone!50] ($(N24)!0.8!(N24.east)$) -- ($(N46)!0.8!(N46.north)$);
                \draw [color=uofgsandstone!50] ($(N34)!0.8!(N34.west)$) -- ($(N15)!0.8!(N15.south)$);
                \draw [color=uofgsandstone!50] ($(N34)!0.8!(N34.west)$) -- ($(N26)!0.8!(N26.east)$);
                \draw [color=uofgsandstone!50] ($(N34)!0.8!(N34.west)$) -- ($(N46)!0.8!(N46.north)$);
                \draw [color=uofgsandstone!50] ($(N44)!0.8!(N44.north)$) -- ($(N15)!0.8!(N15.south)$);
                \draw [color=uofgsandstone!50] ($(N44)!0.8!(N44.north)$) -- ($(N26)!0.8!(N26.east)$);
                \draw [color=uofgsandstone!50] ($(N44)!0.8!(N44.north)$) -- ($(N36)!0.8!(N36.west)$);
                \draw [color=uofgsandstone!50] ($(N15)!0.8!(N15.south)$) -- ($(N26)!0.8!(N26.east)$);
                \draw [color=uofgsandstone!50] ($(N25)!0.8!(N25.east)$) -- ($(N16)!0.8!(N16.south)$);
                \draw [color=uofgsandstone!50] ($(N25)!0.8!(N25.east)$) -- ($(N36)!0.8!(N36.west)$);
                \draw [color=uofgsandstone!50] ($(N25)!0.8!(N25.east)$) -- ($(N46)!0.8!(N46.north)$);
                \draw [color=uofgsandstone!50] ($(N35)!0.8!(N35.west)$) -- ($(N46)!0.8!(N46.north)$);
                \draw [color=uofgsandstone!50] ($(N45)!0.8!(N45.north)$) -- ($(N26)!0.8!(N26.east)$);
                \draw [color=uofgsandstone!50] ($(N45)!0.8!(N45.north)$) -- ($(N36)!0.8!(N36.west)$);
                \draw [ultra thick] ($(N12)!0.8!(N12.south)$) -- ($(N43)!0.8!(N43.north)$);
                \draw [ultra thick] ($(N12)!0.8!(N12.south)$) -- ($(N35)!0.8!(N35.west)$);
                \draw [ultra thick] ($(N12)!0.8!(N12.south)$) -- ($(N26)!0.8!(N26.east)$);
                \draw [ultra thick] ($(N43)!0.8!(N43.north)$) -- ($(N35)!0.8!(N35.west)$);
                \draw [ultra thick] ($(N43)!0.8!(N43.north)$) -- ($(N26)!0.8!(N26.east)$);
                \draw [ultra thick] ($(N35)!0.8!(N35.west)$) -- ($(N26)!0.8!(N26.east)$);
            \end{scope}
        \end{scope}
    \end{tikzpicture}

    \caption{On the left, there is a non-induced isomorphism from the pattern graph, in the center, to the target
    on the right, mapping vertex 1 to 1, 2 to 3, 3 to 5 and 4 to 6. This is not an induced
    isomorphism, since there is an edge between 1 and 5 in the target but not between 1 and 3 in the
    pattern. The mapping from the pattern to the (same) target on the left, sending 1 to 2, 2 to 6, 3 to
    5 and 4 to 3, is both non-induced and induced.
    To the right, the association graph encoding for the induced version: the highlighted clique
    corresponds to the same mapping.}
    \label{figure:sip}
\end{figure}

Despite these successes, subgraph isomorphism algorithms cannot handle \emph{arbitrary} instances
involving hundreds or thousands of vertices. Experimental evaluations of subgraph isomorphism
algorithms are usually performed using a mix of real-world graphs, graphs that encode biochemistry
and computer vision problems, and randomly generated graph pairs. Using random instances to evaluate
algorithm behaviour can be beneficial, because it provides a way of generating many instances
cheaply, and reduces the risk of over-fitting when tuning design parameters. The random instances
used in each case came from common datasets \citep{DeSanto:2003,Zampelli:2010}, which were generated
by taking a random subgraph of a random (Erd\H{o}s-R\'enyi, scale-free, bounded degree, or mesh)
graph and permuting the vertices. Such instances are guaranteed to be
satisfiable---\citet{Anton:2009} exploited this property to create large sets of random satisfiable
boolean satisfiability instances.  This has been by far the most common approach used so far to
generate random subgraph isomorphism instances so far, meaning existing benchmark suites contain
relatively few non-trivial unsatisfiable instances.  Also, the satisfiable instances tend to be
computationally fairly easy, with most of the difficulty being in dealing with the size of the
model. This has lead to bias in algorithm design, to the extent that some proposed techniques will
\emph{only} work on satisfiable instances \citep{Battiti:2007}.

Our first contribution is to present and evaluate a new method for creating random pattern/target
pairs. This method generates both satisfiable and unsatisfiable instances, and can produce
computationally challenging instances with only a few tens of vertices in the pattern, and 150
vertices in the target. This is not entirely straightforward---the lack of unsatisfiable instances
for testing purposes cannot be addressed simply by using taking a pattern graph from one of the
existing random suites with the ``wrong'' target graph, as this tends to give either a trivially
unsatisfiable instance, or a satisfiable instance. (In particular, it is \emph{not} the case that a
relatively small random graph is unlikely to appear in a larger random graph.)

This work builds upon the phase transition phenomena observed in satisfiability and graph colouring
problems first described by \citet{DBLP:conf/ijcai/CheesemanKT91} and \citet{Mitchell:1992}.  For
subgraph isomorphism we identify three relevant control parameters: we can independently alter the
edge probability of the pattern graph, the edge probability of the target graph, and the relative
orders (number of vertices) of the pattern and target graphs.  For non-induced isomorphisms, with
the correct choice of parameters we see results very similar to those observed with boolean
satisfiability problems: there is a phase transition (whose location we can predict) from
satisfiable to unsatisfiable, we see a solver-independent complexity peak occur near this phase
transition, and understanding this behaviour helps us to select variable and value ordering
heuristics.

For certain choices of parameters for induced isomorphisms, there are two phase transitions, going
from satisfiable to unsatisfiable, and then from unsatisfiable back to satisfiable. Again, when
going from satisfiable to unsatisfiable (from either direction), instances go from being trivial to
really hard to solve. However, each of the three solvers we tested also finds the central
unsatisfiable region to be hard, despite it not being near a phase transition. To show that this is
not a simple weakness of current subgraph isomorphism algorithms, we verify that this region is also
hard when using a pseudo-boolean encoding, and under reduction to the clique problem. Interestingly,
the constrainedness measure proposed by \citet{Gent:1996:Kappa} \emph{does} predict this difficult
region---these instances provide evidence in favour of constrainedness, rather than proximity to a
phase transition, being an accurate predictor of difficulty, and show that constrainedness is not
simply a refinement of a phase transition prediction.

When labels on vertices are introduced, as is commonly seen in graph database systems, richer
behaviour emerges. We look at some of the datasets commonly used to test graph database systems,
which the literature suggests are extremely hard to solve. Our theory predicts that these instances
should be trivial, and indeed we see that the entire perceived difficulty of every dataset we
investigate comes down to the widespread use of a particular subgraph isomorphsim algorithm which
does not use a theoretically-strong variable ordering heuristic. Our second contribution is to show
that the use of the right variable ordering heuristic, even without the more sophisticated recent
advances in subgraph algorithms (which have been largely neglected outside of the artificial
intelligence community), makes the entire graph database filter / verify paradigm unnecessary.
Finally, we explain why filter / verify cannot be beneficial even on more challenging instances
except if an extremely poor choice of subgraph isomorphism algorithm is made.


\subsection{Definitions}

Throughout, our graphs are undirected, and do not have any loops.  The \emph{order} of a graph is
the cardinality of its vertex set. We write $\operatorname{V}(G)$ for the vertex set of a graph $G$.
The \emph{complement} of a graph $G$, denoted $\shortoverline{G}$, is the graph with the same vertex
set as $G$, and with an edge between distinct vertices $v$ and $w$ if and only if $v$ and $w$ are
not adjacent in $G$. We write $G(n, p)$ for an Erd\H{o}s-R\'enyi random graph with $n$ vertices, and
an edge between each distinct pair of vertices with independent probability $p$.  A graph may have
labels on its vertices, in which case we write $\ell(v)$ for the label of a given vertex $v$.

A \emph{non-induced subgraph isomorphism} from a graph $P$ (called the \emph{pattern}) to a graph
$T$ (called the \emph{target}) is an injective mapping $i$ from $\operatorname{V}(P)$ to
$\operatorname{V}(T)$ which preserves adjacency---that is, for every adjacent $v$ and $w$ in
$\operatorname{V}(P)$, the vertices $i(v)$ and $i(w)$ are adjacent in $T$. If vertices are labelled,
the mapping must preserve labels, so $\ell(v) = \ell(i(v))$. An \emph{induced subgraph isomorphism}
additionally preserves non-adjacency---that is, if $v$ and $w$ are not adjacent in $P$, then $i(v)$
and $i(w)$ must not be adjacent in $T$. We use the notation $i : P \rightarrowtail T$ for a
non-induced isomorphism, and $i : P \hookrightarrow T$ for an induced isomorphism. Observe that an
induced isomorphism $i : P \hookrightarrow T$ is a non-induced isomorphism $i : P \rightarrowtail T$
which is also a non-induced isomorphism $\shortoverline{i} : \shortoverline{P} \rightarrowtail
\shortoverline{T}$.

\subsection{Experimental Setup}

Our experiments were performed on systems with Intel Xeon E5-4650 v2 CPUs, running Scientific Linux
release 6.7. We selected three subgraph isomorphism solvers: the Glasgow solver
\citep{McCreesh:2015}, LAD \citep{Solnon:2010}, and VF2 \citep{Cordella:2004}; each was compiled
using GCC 4.9.

The Glasgow and LAD solvers use backtracking search to build up an assignment of target vertices
(values) to pattern vertices (variables), but differ in terms of inference and ordering heuristics.
The approach used by VF2 is similar, although the domains of variables are not stored (in the style
of conventional backtracking, rather than forward-checking), and so domain wipeouts are not detected
until an assignment is made.

In each case we measure the number of recursive calls (guessed assignments) made, not runtimes. We
are not aiming to compare absolute performance between solvers; rather, we are looking for
solver-independent patterns of difficulty. We used a timeout of 1,000 seconds, which was enough for
the Glasgow solver to solve nearly all our instances (whose orders were selected with this timeout
in mind), although we may slightly overestimate the proportion of unsatisfiable instances for
extremely sparse or dense pattern graphs. The LAD and VF2 solvers experienced many more failures
with this timeout, so our picture of just how hard the hardest instances are with these solvers is
less detailed.

\section{Non-Induced Subgraph Isomorphisms}

\begin{figure}[tb]
    \centering\includegraphics{gen-graph-phase-transition.pdf}
    \caption{With a fixed pattern graph order of 20, a target graph order of 150, a target edge
        probability of 0.40, and varying pattern edge probability, we observe a phase transition and
        complexity peak with the Glasgow solver in the non-induced variant. Each point represents
        one instance. The lines show mean search effort and mean proportion satisfiable.}
    \label{figure:phase-transition}
\end{figure}

Suppose we arbitrarily decide upon a pattern graph order of 20, a target graph order of 150, a fixed
target edge probability of 0.40, and no vertex labels. As we vary the pattern edge probability from
0 to 1, we would expect to see a shift from entirely satisfiable instances (with no edges in the
pattern, we can always find a match) to entirely unsatisfiable instances (a maximum clique in this
order and edge probability of target graph will usually have between 9 and 12 vertices). The dark
line in \cref{figure:phase-transition} shows that this is the case. For densities of 0.67 or
greater, no instance is satisfiable; with densities of 0.44 or less, every instance is satisfiable;
and with a density of 0.55, roughly half the instances are satisfiable.

The light line plots mean search effort using the Glasgow solver: for sparse patterns, the problem
is trivial, for dense patterns proving unsatisfiability is not particularly difficult, and we see a
complexity peak around the point where half the instances are satisfiable.  We also plot the search
cost of individual instances, as points. The behaviour we observe looks remarkably similar to random
3SAT problems---compare, for example, Figure 1 of \citet{LeytonBrown:2014}. In particular,
satisfiable instances tend to be easier, but show greater variation than unsatisfiable instances, and
there are exceptionally hard satisfiable instances \citep{Smith:1997}. (The Glasgow solver supports
parallel search with a work-stealing strategy explicitly designed to eliminate these. We have not
enabled this option to avoid dealing with the complexity of search tree measurements under
parallelism.)

\begin{figure}[tb]
    \centering\includegraphics{gen-graph-non-induced.pdf}
    \caption{Behaviour of algorithms on the non-induced variant. For each plot, the x-axis is the
        pattern edge probability and the y-axis is the target edge probability, both from 0 to 1.
        Along the top row, we show the proportion of instances which are satisfiable; the white
        bands shows the phase transitions, and the black lines are our predictions of where the
        phase transition will occur. On the final three rows, we show the number of search nodes used by the
        Glasgow, LAD and VF2 solvers; the dark regions indicate ``really hard'' instances.}
    \label{figure:non-induced}
\end{figure}

What if we alter the edge probabilities for both the pattern graph and the target graph?  In the top
row of \cref{figure:non-induced} we show the satisfiability phase transition for the non-induced
variant, for patterns of order 10, 20 and 30, targets of order 150, and varying pattern (x-axis) and
target (y-axis) edge probabilities. Each axis runs over 101 edge probabilities, from 0 to 1 in steps
of 0.01. For each of these points, we generate ten random instances. The colour denotes the
proportion of these instances which were found to be satisfiable.  Inside the orange region, at the
bottom right of each plot, every instance is unsatisfiable---here we are trying to find a dense
pattern in a sparse target. In the purple region, at the top left, every instance is
satisfiable---we are looking for a sparse pattern in a dense target (which is easy, since we only
have to preserve adjacency, not non-adjacency). The white band between the regions shows the
location of the phase transition: here, roughly half the instances are satisfiable. (We discuss the
black line below.)

On subsequent rows, we show the average number of search nodes used by the different algorithms. In
general, satisfiable instances are easy, until very close to the phase transition. As we hit the
phase transition and move into the unsatisfiable region, we see complexity increase. Finally, as
we pass through the phase transition and move deeper into the unsatisfiable region, instances become
easier again. This behaviour is largely solver-independent, although VF2 has a larger hard region
than Glasgow or LAD (and the observant reader may notice that VF2 finds some instances extremely
hard when the target is empty but the pattern is not---this turns out to be extremely important, and
we return to it in \cref{section:labelled,section:filterverify}). Thus, although we have moved away
from a single control parameter, we still observe the easy-hard-easy pattern seen in many other
\NP-complete problems.

\subsection{Locating the Phase Transition}

We can approximately predict the location of the phase transition by calculating (with
simplifications regarding rounding and independence) the expected number of solutions for given
parameters. Since we are trying to find an \emph{injective} mapping from a pattern $P = G(p, d_p)$
to a target $T = G(t, d_t)$, there are \[ t^{\underline{p}} = t \cdot (t - 1) \cdot \ldots \cdot (t -
p + 1) \] possible assignments of target vertices to pattern vertices.  We expect the pattern to have
$d_p \cdot \binom{p}{2}$ edges, so we obtain the probability of all of these edges being mapped to
edges in the target by raising $d_t$ to this power, giving an expected number of solutions of \[
\langle Sol \rangle = t^{\underline{p}} \cdot {d_t}^{d_p \cdot \binom{p}{2}} \textnormal{.} \] This
formula predicts a very sharp phase transition from $\langle Sol \rangle \ll 1$ to $\langle Sol
\rangle \gg 1$, which may easily be located numerically. We plot where this occurs using black lines
in the first row of \cref{figure:non-induced}.

This prediction is generally reasonably accurate, except that for very low and very high pattern
densities, we overestimate the satisfiable region. This is due to variance: although an expected
number of solutions much below one implies a high likelihood of unsatisfiability, it is not true
that a high expected number of solutions implies that any particular instance is likely to be
satisfiable. (Consider, for example, a sparse graph which has several isolated vertices. If one
solution exists, other symmetric solutions can be obtained by permuting the isolated vertices.
Thus although the expected number of solutions may be one, there cannot be exactly one solution.) A
similar behaviour is seen with random constraint satisfaction problems
\citep{Smith:1996}.

\subsection{Variable and Value Ordering Heuristics}

Various general principles have been considered when designing variable and value ordering
heuristics for backtracking search algorithms---one of these is to try to maximise the expected
number of solutions inside any subproblem considered during search \citep{Gent:1996:EN}.  This is
usually done by cheaper surrogates, rather than direct calculation. When branching, both LAD and
Glasgow pick a variable with fewest remaining values in its domain: doing this will generally reduce
the first part of the $\langle Sol \rangle$ equation by as little as possible. When two or more
domains are of equal size, LAD simply breaks ties lexicographically, whereas Glasgow will pick a
variable corresponding to a pattern vertex of highest degree. This strategy was determined
empirically, but could have been derived from the $\langle Sol \rangle$ formula: picking a pattern
vertex of high degree will make the remaining pattern subgraph sparser, which will decrease the
exponent in the second half of the formula, maximising the overall value. LAD does not apply a value
ordering heuristic, but Glasgow does: it prefers target vertices of lowest degree.  Again, this was
determined empirically, but it has the effect of increasing $\langle Sol \rangle$ by increasing the
remaining target density. The VF2 heuristics, in contrast, are based around preserving connectivity,
which gives very little discrimination except on the sparsest of inputs.

\section{Induced Subgraph Isomorphisms}

\begin{figure}[p]
    \centering\includegraphics{gen-graph-induced.pdf}
    \caption{Behaviour of algorithms on the induced variant, shown in the style
    of \cref{figure:non-induced}. The fifth row plots constrainedness: the darkest region is where
    $\kappa = 1$, and the lighter regions show where the problem is either over-
    or under-constrained. The final row shows when the Glasgow algorithm performs better when given
    the complements of the pattern and target graphs as inputs---the solid lines show the location
of the phase transition, and the dotted lines are $t_d=0.5$ and the $p_d=t_d$ diagonal.}\label{figure:induced}
\end{figure}

\begin{figure}[tb]
    \centering\includegraphics{gen-graph-sat.pdf}
    \caption{Behaviour of other solvers on the induced variant on smaller graphs, shown in the style of
        \cref{figure:non-induced}. The second row shows the number of search nodes used by the
    Glasgow algorithm, the third and fourth rows show the number of decisions made by the pseudo-boolean
and SAT solvers, and the final shows the number of search nodes used on the clique
encoding.}\label{figure:alt}
\end{figure}

In the first four rows of \cref{figure:induced} we repeat our experiments, finding induced
isomorphisms. With a pattern of order 10, we get two independent phase transitions: the bottom right
half of the plots resemble the non-induced results, and the top left half is close to a mirror
image. The central satisfiable region, which is away from either phase transition, is
computationally easy, but instances near the phase transition are hard.

For larger patterns of order 20 and 30, we have a large unsatisfiable region in the middle. Despite
not being near either phase transition, instances in the centre remain computationally challenging.
We also plot patterns of orders 14, 15 and 16, to show the transition between the two behaviours.

\subsection{Predictions and Heuristics}

To predict the location of the induced phase transition, we repeat the argument for locating the
non-induced phase transition and additionally considering non-edges, to get an expected number of
solutions of \[ \langle Sol \rangle = t^{\underline{p}} \cdot {d_t}^{d_p \cdot \binom{p}{2}} \cdot
{(1 - d_{t})}^{(1 - d_{p}) \cdot \binom{p}{2}} \textnormal{.} \] We plot this using black lines on
the top row of \cref{figure:induced}---again, our prediction is accurate except for very sparse or
very dense patterns.

We might guess that degree-based heuristics would just not work for the induced problem: for any
claim about the degree, the opposite will hold for the complement constraints. However, empirically,
this is not the case: on the final row of \cref{figure:induced}, we show whether it is better to use
the original pattern and target as the input to the Glasgow algorithm, or to take the complements.
(The only steps performed by the Glagsow algorithm which differ under taking the complements are
the degree-based heuristics.  LAD and VF2 are not symmetric in this way: LAD performs a filtering
step using degree information, but does not consider the complement degree, and VF2 uses
connectivity in the pattern graph.)

For patterns of order 10, it is always better to try to move towards the satisfiable region: if we
are in the bottom right diagonal half, we are best retaining the original heuristics (which move us
towards the top left), and if we are in the top left we should use the complement instead. This
goes against a suggestion by \citet{Walsh:1998} that switching heuristics based upon an estimate of
the solubility of the problem may offer good performance.

For larger patterns, more complex behaviour emerges. If we are in the intersection of the bottom half
and the bottom right diagonal of the search space, we should always retain the original heuristic,
and if we are in the intersection of the top half and the top left diagonal, we should always use
the complements. This behaviour can be predicted by taking the partial derivatives of $\langle Sol
\rangle$ in the $-p_d$ and $t_d$ directions.  However, when inside the remaining two eighths of the
parameter space, the partial derivatives of $\langle Sol \rangle$ disagree on which heuristic to
use, and using directional derivatives is not enough to resolve the problem. A close observation of
the data suggests that the actual location of the phase transition may be involved (and perhaps
\citeS{Walsh:1998} suggestion applies only in these conditions). In any case, $\langle Sol
\rangle$ is insufficient to explain the observed behaviour in these two eighths of the parameter space.

In practice, this is unlikely to be a problem: most real-world instances are extremely sparse and
are usually easy, which perhaps explains the continuing popularity of VF2's connectivity-based
heuristics \citep{Carletti:2015}. In this situation, these experiments justify reusing the
non-induced heuristics on induced problems.

\subsection{Is the Central Region Genuinely Hard?}

The region in the parameter space where both pattern and target have medium density is far from a
phase transition, but nevertheless contains instances that are hard for all three solvers. We would
like to know whether this is due to a weakness in current solvers (perhaps our solvers cannot reason
about adjacency and non-adjacency simultaneously?), or whether instances in this region are
inherently difficult to solve.  Thus we repeat the induced experiments on smaller pattern and target
graphs, using different solving techniques.  Although these techniques are not competitive in
absolute terms, we wish to see if the same pattern of behaviour occurs. The results are plotted in
\cref{figure:alt}.

The pseudo-boolean (PB) encoding is as follows. For each pattern vertex $v$ and each target vertex
$w$, we have a binary variable which takes the value 1 if and only if $v$ is mapped to $w$.
Constraints are added to ensure that each pattern vertex maps to exactly one target vertex, that
each target vertex is mapped to by at most one pattern vertex, that adjacent vertices are mapped to
adjacent vertices, and that non-adjacent vertices are mapped to non-adjacent vertices. We used the
Clasp solver~\citep{gekakaosscsc11a} version 3.1.3 to solve the pseudo-boolean instances.  The
instances that are hard for the Glasgow solver remain hard for the PB solver, including instances
inside the central region, and the easy satisfiable instances remain easy. Similar results were
seen with the Glucose SAT solver \cite{glucose} using a direct encoding of the cardinality constraints. (We also
implemented an integer program encoding; the Gurobi solver was only able to solve some of the
trivial satisfiable instances, and was almost never able to prove unsatisfiability within the time
limit.)

The \emph{association graph encoding} of a subgraph isomorphism problem (illustrated in
\cref{figure:sip}) is constructed by creating a new graph with a vertex for each pair $(p, t)$ of
vertices from the pattern and target graphs respectively. There is an edge between vertex $(p_1,
t_1)$ and vertex $(p_2, t_2)$ if mapping $p_1$ to $t_1$ and $p_2$ to $t_2$ simultaneously is
permitted, i.e.\ $p_1$ is adjacent to $p_2$ if and only if $t_1$ is adjacent to $t_2$. A clique of
size equal to the order of the pattern graph exists in the association graph if and only if the
problem is satisfiable \citep{Levi:1973}. We used this encoding with the BBMC clique algorithm
\citep{SanSegundo:2011}, which we implemented in C++. Usually BBMC solves the optimisation version
of the clique problem; we adapted it to solve the decision problem by initialising the incumbent to
be one less than the decision value, and allowing it to exit as soon as a clique with size equal to
the decision value is encountered.  Again, our results show that the instances in the central region
remain hard, and additionally, some of the easy unsatisfiable instances become hard.

Together, these experiments suggest that the central region may be genuinely hard, despite not being
near a phase transition. The clique results in particular rule out the hypothesis that subgraph
isomorphism solvers only find this region hard due to not reasoning simultaneously about adjacency
and non-adjacency, since the association graph encoding constraints consider compatibility rather
than adjacency and non-adjacency.

\subsection{Constrainedness}

Constrainedness, denoted $\kappa$, is an alternative measure of difficulty designed to refine the
phase transition concept, and to generalise hardness parameters across different combinatorial
problems \citep{Gent:1996:Kappa}. A problem with $\kappa < 1$ is said to be underconstrained, and is
likely to be satisfiable; a problem with $\kappa > 1$ is overconstrained, and is likely to be
unsatisfiable. Empirically, problems with $\kappa$ close to 1 are hard, and problems where $\kappa$
is very small or very large are usually easy. By handling injectivity as a restriction on the size
of the state space rather than as a constraint, we derive \[ \kappa = 1 - \frac{\log \left(
t^{\underline{p}} \cdot {d_t}^{d_p \cdot \binom{p}{2}} \cdot {(1 - d_{t})}^{(1 - d_{p}) \cdot
\binom{p}{2}} \right)}{\log t^{\underline{p}}}\] for induced isomorphisms, which we plot on the
fifth row of \cref{figure:induced}. We see that constrainedness predicts that the central region
will still be relatively difficult for larger pattern graphs: although the problem is
overconstrained, it is less overconstrained than in the regions the Glasgow and LAD solvers found
easy.  Thus it seems that rather than just being a unification of existing generalised heuristic
techniques, constrainedness also gives a better predictor of difficulty than proximity to a phase
transition---our method generates instances where constrainedness and ``close to a phase
transition'' give very different predictions, and constrainedness gives the better prediction.

Unfortunately, constrainedness does not help us with heuristics: minimising constrainedness gives
the same predictions as maximising the expected number of solutions.

\section{Other Models of Randomness}\label{section:models}

\section{Labelled Graphs}\label{section:labelled}

\begin{figure}[t]
    \centering\includegraphics{gen-graph-labels.pdf}
    \caption{On the top row, predicted and actual location of the phase transition for labelled
    non-induced random subgraph isomorphism, with a pattern order of 20, a target order of 150,
    varying pattern (x-axis) and target (y-axis) density, and varying numbers of labels. On
    subsequent rows, the average number of search nodes needed to solve an instance, for three
    different solvers.}\label{figure:labels}
\end{figure}

So far, we have looked at unlabelled graphs. What happens when labels on vertices are introduced?
This is common in real-world applications---for example, when working with graphs representing
chemical molecules, mappings are typically expected only to match carbon atoms with carbon atoms,
hydrogen atoms with hydrogen atoms, and so on.  We will look at the non-induced variant, as this
seems to be more common in the literature, and return to the Erd\H{o}s-R\'enyi model.

Suppose our labels are drawn randomly from a set $L = \{ 1, \ldots, k \}$, where $k$ is reasonably
small compared to $p$. We can partition the pattern vertices by label into disjoint sets \[\{
    \operatorname{V}(P)\vert_1 = \{ v \in \operatorname{V}(P) : \ell(v) = 1 \}, \ldots,
    \operatorname{V}(P)\vert_k \}\textnormal{\,,}\] each of which is expected to contain
$\nicefrac{p}{k}$ vertices. Similarly, we may partition the target vertices into disjoint sets $\{
    \operatorname{V}(T)\vert_1, \ldots, \operatorname{V}(T)\vert_k \}$.

Without labels, there are $t^{\underline{p}} = t \cdot (t - 1) \cdot \ldots \cdot (t - p + 1)$
possible injective assignments of target vertices to pattern vertices.  With labels, observe that
for any label $x$, vertices in $\operatorname{V}(P)|_x$ may only be mapped to vertices in
$\operatorname{V}(T)|_x$.  Thus for each label $x$, we have an expected $\nicefrac{p}{k}$ variables,
each of whose domains contain $\nicefrac{t}{k}$ values. We would like to say that the size of the
state space is now \[ |S| = \left((\nicefrac{t}{k})^{\underline{\nicefrac{p}{k}}}\right)^{k}
\textnormal{,} \] but to do this we must state what $a^{\underline{b}}$ means when $b$ is fractional.
A reasonable continuous extension may be obtained by taking \[ |S| = \left(\frac{\Gamma\left(\nicefrac{t}{k}
+ 1\right)}{\Gamma\left(\nicefrac{t}{k} - \nicefrac{p}{k} + 1\right)}\right)^{k} \textnormal{\,.} \]

As before, we expect the pattern to have $d_p \cdot \binom{p}{2}$ edges, so we obtain the
probability of all of these edges being mapped to edges in the target by raising $d_t$ to this
power, giving an expected number of solutions of \[ \langle Sol \rangle = \left(
    \frac{\Gamma\left(\nicefrac{t}{k} + 1\right)}{\Gamma\left(\nicefrac{t}{k} - \nicefrac{p}{k} +
1\right)} \right)^{k}  \cdot
{d_t}^{d_p \cdot \binom{p}{2}} \textnormal{\,.} \]

So how good are these predictions? We draw some heatmaps in \cref{figure:labels}. For small numbers
of labels, our predictions are slightly better than in the unlabelled case: there seems to be less
of a variance problem for very dense patterns. Even as the numbers of labels becomes relatively
large, the prediction of the phase transition still occurs in the right place, but we start to see
sporadic unsatisfiable instances deep inside the satisfiable reason. With sufficiently many labels,
we get a kind of phase transition from ``all unsatisfiable'' to ``mixed satisfiable and
unsatisfiable''.

\subsection{What's Wrong with VF2?}

?? What goes on here? With labels, there are two regions where VF2 often finds instances really
hard, but Glasgow and LAD do not. Looking at the data more closely, although the average runtime is
high, there is a wide distribution of runtimes in these areas, and VF2 does sometimes find these
instances trivial too.

?? Give examples here: VF2 does not detect simple things like there being two red vertices in the
pattern but one in the target, or even no red vertices in the target. In contrast, Glasgow and LAD
do. Explain thrashing.

?? Explain what's going on in the satisfiable region. This is a bit more subtle: is it to do with
neighbours, rather than a root node property? Would be nice if we could find a representative
example.

?? We call these \emph{spuriously hard} instances, to distinguish them from \emph{really hard}
problems. Not the same as \emph{exceptionally hard}.

?? Look more at size of min unsat core in these areas?

\subsection{Spuriously Hard Instances?}

?? Use of VF2 in other areas.

?? Need for better benchmark instances, not running millisecond versus microsecond benchmarks.

?? However, by far the biggest problem is in graph databases. Not just lead to overly pessimistic
conclusions, but has misdirected the design of systems. Note that these deal with sparse, non-random
graphs.

\section{Querying Graph Databases}\label{section:filterverify}

The general problem is, for a set of target graphs, to process a pattern query and return every
target graph which is subgraph-isomorphic to that pattern. The set of target graphs is usually seen
as fixed, or at least rarely-changing, whilst the patterns arrive dynamically. This has lead to the
development of systems which perform extensive computations on the target graphs beforehand, in the
hopes of reducing the response times for individual pattern queries. The most popular of these
strategies is a form of indexing which is often named \emph{filter / verify}.

\subsection{The Filter / Verify Paradigm}

The filter / verify approach has an interesting history, of which we now give a very selective and
incomplete overview. Our description is biased by a general modern understanding of \NP-completeness
which was not widely known at least at the time of the earlier papers we discuss. The common theme
of all of the following papers is that pre-computed information is used to eliminate certain
unsatisfiable instances from consideration, without performing a subgraph isomorphism test.

?? Give a quick example here of how an index would eliminate the two red vertices in the pattern but
one red vertex in the target example.

An early graph database system by \citet{DBLP:conf/pods/ShashaWG02} uses a filtering heuristic to
eliminate unsatisfiable instances based upon simple structural elements. It is not clear whether the
aim is to reduce I/O costs, or to reduce the number of queries which must be tested, and the
experiments do not answer whether the indexing is effective. However, the work was influenced by a commercial
graph database system, whose documentation \cite[section 7.1]{Daylight} states that indexing is
used to minimise disk accesses.

Subsequently, in a widely cited paper, \citet{DBLP:conf/sigmod/YanYH04} introduce an indexing system
called gIndex. Again, this system handles queries by first producing a set of candidates by
eliminating certain unsatisfiable instances, this time by using substructures. They argue that the
query response time, which is to be minimised, is governed by the equation \[ T =
T_{\mathit{search}} + \left|C_q\right| \cdot T_{\mathit{iso\_test}} \textnormal{,}\] where
$T_{\mathit{search}}$ is the time taken to search for a candidate set of potential solutions of size
$\left|C_q\right|$, and $T_{\mathit{iso\_test}}$ is the cost of a subgraph isomorphism test. They
reason that since isomorphism testing is \NP-complete, by making $\left|C_q\right|$ as small as
possible, the query response time will be reduced.  (Importantly, it is \emph{not} time taken to
load graphs from disk which contributes to the per-candidate cost, but rather the time to perform a
subgraph isomorphism call.) They conclude that ``graph indexing plays a critical role at efficient
query processing in graph databases''.

This equation is repeated and expanded upon by \citet{DBLP:journals/tods/YanYH05} to explicitly
separate I/O and isomorphism testing costs, obtaining a query response time of \[
    T_{\mathit{search}} + \left|C_q\right| \cdot (T_{\mathit{io}} + T_{\mathit{iso\_test}})
    \textnormal{.} \] The authors explicitly state that ``the value of $T_{\mathit{iso\_test}}$ does
not change much for a given query''. The argument presented is that
\begin{displayquote}``Sequential scan is very costly
because one has to not only access the whole graph database but also check subgraph isomorphism. It
is known that subgraph isomorphism is an \NP-complete problem. Clearly, it is necessary to build
graph indices in order to help processing graph queries.''\end{displayquote}

With what we now know about the behaviour of modern subgraph isomorphism algorithms, and the nature
of solving \NP-complete problems in general, we should immediately be suspicious of these claims. We
do not expect $T_{\mathit{iso\_test}}$ to be anything like a constant, even if the orders of the
input graphs are similar. In particular, any instance which can be excluded based upon filtering
must have a very small proof of unsatisfiability. These instances \emph{should} be trivial with any
decent matching algorithm. Thus, all filtering \emph{should} be doing is eliminating the startup
costs of a trivial subgraph isomorphism call. The fact that filtering was successful empirically
should make us wonder whether the subgraph isomorphism algorithm being used was excessively
primitive. Indeed, the matching algorithm used is described only as ``the simplest approach'' in the
paper.  Additionally, the experiments focus on reducing the size of the candidate set, without
considering the time taken to verify different candidate set instances.  The claim that
$T_{\mathit{iso\_test}}$ does not change much is not justified experimentally, and no consideration
is given as to whether this would hold true for other subgraph matching algorithms.

Moving forwards, the (simpler form of the) query response time equation is repeated by
\citet{DBLP:conf/vldb/ZhaoYY07}. Again, the work has a focus on reducing the candidate set size
through indexing. The authors appear to believe that the cost of the isomorphism test is not a major
factor in influencing the result, and focus on the remaining terms in the equation. They use the
``average cost'' of a subgraph isomorphism test as a constant, without considering that the average
cost could be influenced by the character of the candidate set.

The equation is also used by \citet{DBLP:conf/icde/JiangWYZ07}, who claim that ``usually the
verification time dominates the Query Response Time [s]ince the computational complexity of
$T_{\mathit{iso\_test}}$ is NP-Complete''. They note that \begin{displayquote}``Approximately, the
    value of $T_{\mathit{iso\_test}}$ does not change too much with the difference of query. Thus,
the key to reducing query response time is to minimize the size of the candidate answer
set''.\end{displayquote} This claim becomes understandable when one examines the subgraph matching
algorithm chosen for the verification step \citep{DBLP:journals/jacm/Ullmann76}: as the algorithm
predates techniques like arc-consistent all-different propagation \citep{DBLP:conf/aaai/Regin94}, it
cannot immediately detect unsatisfiability in simple cases like there being two red vertices in the
pattern but only one in the target\footnote{?? expand on: Although interestingly, this algorithm
effectively does forward-checking and decent value ordering heuristics before these concepts
appeared in the constraints literature.}.

Without using the equation, \citet{DBLP:conf/sigmod/ChengKNL07} state that since subgraph
isomorphism is NP-complete, processing by a sequential scan is infeasible. They introduce new
filtering techniques to try to avoid the subgraph isomorphism step. A similar claim is made by
\citet{DBLP:conf/icde/ZhangHY07} in a introduction of another indexing technique: ``obviously it is
inefficient to perform a sequential scan on every graph in the database, because the subgraph
isomorphism test is expensive''.

Muddying the waters slightly, a survey by \citet{DBLP:journals/datamine/HanCXY07} states that
``large volumes of data'' (not \NP-completeness) is the reason for using indexing in these systems.
However, in a description of a system tailored to biological networks,
\citet{DBLP:conf/edbt/ZhangLY09} state that \begin{displayquote}``Since the size of the raw database
    graph is small, it can be easily fit in the main memory.  However, the query (matching) time
will be very long due to the NP-hard complexity.''\end{displayquote} They suggest that indexing is
a way of avoiding this.

\citet{DBLP:journals/tkde/WangWYY12} look at indexing large sparse graphs. They state that ``because
subgraph isomorphism is an \NP-complete problem, a filter-and-verification method is usually
employed to speed up the search efficiency of graph similarity matching over a graph set''.
Similarly, after reviewing the literature, \citet{DBLP:journals/vldb/YuanM13} argue that subgraph
querying is costly because it is \NP-complete, and that indices can improve the performance of graph
database queries. Again, new indexing techniques are introduced.  Subsequently,
\citet{DBLP:journals/pvldb/KatsarouNT15} perform a comprehensive comparison of graph database
filtering techniques. They state that performing a query against each graph in the dataset
``obviously does not scale, as subgraph isomorphism is \NP-complete''.

The filter / verify paradigm also influences other research. For example, recently
\citet{DBLP:journals/pvldb/YuanMG13} continue a line of supergraph search work, again using a
filtering step to avoid subgraph isomorphism calls. There is also research into maintaining indices
when the set of target graph changes: for example \citet{DBLP:conf/sigmod/YuanMYG15} look at
algorithms for updating graph indices. And in describing a system for reusing results of queries
which are sub- or super-graphs of previous queries, \citet{DBLP:conf/edbt/WangNT16} state that
querying is a ``very costly operation as it entails the \NP-complete problem of subgraph
isomorphism'', and place ``an emphasis on the number of unnecessary subgraph isomorphism tests''.

After some early ambiguity, it becomes clear that the intent behind filter / verify systems is to
reduce the number of subgraph isomorphism calls, and that the cost of loading graphs from disk is
not considered to be problematic. It is worth noting that the entire test datasets from most of
these papers will comfortably fit in RAM on a modern desktop machine, even when an adjacency matrix
representation is used.

Thus we can see that there are two critical beliefs underlying all of this work---firstly, that
subgraph isomorphism is necessarily hard because it is \NP-complete, and secondly, that there are
ways of identifying unsatisfiable instances using short proofs that a subgraph isomorphism algorithm
will not detect, but that an indexing system can. Throughout, the cost models used assume that the
time for a subgraph isomorphism query does not particularly depend upon the instance, and nowhere is
it considered that a good subgraph isomorphism algorithm should be able to eliminate
obviously-unsatisfiable instances with a similar time requirement to an indexing system.

These beliefs are not entirely unfounded: none of the matching algorithms considered in these papers
will immediately detect if a pattern contains two red vertices, whilst the target graph contains
only one. This kind of flaw \emph{should} be picked up at the top of search by an all-different
propagator \citep{DBLP:conf/aaai/Regin94}.  However, as \citet{DBLP:journals/pvldb/KatsarouNT15}
note, some variation on VF2 \citep{Cordella:2004} is the usual matching algorithm of choice for
graph database systems, although \citeS{DBLP:journals/jacm/Ullmann76}'s algorithm is sometimes
chosen. Other approaches have been considered: for example, \citet{DBLP:journals/pvldb/ShangZLY08}
propose an algorithm which makes use of the frequency of various features to guide search.
\citet{DBLP:journals/pvldb/LeeHKL12} determine experimentally that this technique tends to be very
effective, even on families of graphs for which it was not designed.) We believe it would be
unlikely to cause too much astonishment if we suggested that a better matching algorithm could be
dropped in as a black box replacement in graph databases systems to improve their performance.
However, it appears not to have occurred to the graph databases community that a better subgraph
isomorphism algorithm could invalidate the premise underlying the entire filter / verify approach.

?? Eliminating this filtering step would not merely be a matter of simplicity: indices take ??  hours
to build.

?? Additionally, this stuff works on satisfiable instances too. Indexing addresses some of the
obviously-unsatisfiable cases that VF2 struggles with, but does nothing on the satisfiable
instances.

\subsection{An Algorithm which Requires No Filtering}

?? We build up a simple algorithm. Use smallest domain first, all different, NDS, possibly distance
2 filtering. Show its effectiveness at eliminating unsatisfiable instances from graph database test
sets, and give its total performance.  None of the steps in this algorithm are new. Our aim in this
section is to show which aspects of modern algorithm design are critical, and that an indexless
sequential scan is entirely plausible with a decent algorithm.

?? To tie this into earlier parts of the paper, we also show what happens if we use the VF2
heuristic in this setting, rather than smallest domains first.

?? This stuff eliminates all the spuriously hard unsatisfiable instances along the bottom of the
graph, but also helps on the satisfiable instances that an indexing system cannot touch.

\subsection{What Should the Future of Graph Matching in Databases Be?}

?? Clearly lots of scope for cooperation between AI and graph databases. AI is not just good for
better algorithms, but the design of entire systems.

?? We are not presenting the ultimate big data subgraph matching algorithm.

?? We need optimisations for trivial cases. Lazy forward checking, cheap alternatives to heuristics
and all-different. Presolving. Huge domains.  Precalculating NDS, distance 2 etc, not for indexing.
Edge labels.

?? Degrade to clique. Symmetries.

?? Invariant which are effective on labels used to inform algorithm design and to provide filtering
during search. Look at minimum unsat core. If we are prepared to spend hours, indexing-like system
can be used to enhance propagation, for example by pre-computing degree sequences and paths on
target graphs. This is stronger, because, say, counting red vertices and counting degrees are
automatically combined.

?? On the other hand, our key message is not just saying use a better algorithm. Need a full
understanding of every aspect of the system, and cannot treat subgraph matching as a black box.

\section{Conclusion}

We have shown how to generate small but hard instances for the non-induced and induced subgraph
isomorphism problems, which will help offset the bias in existing datasets. For non-induced
isomorphisms, behaviour was as in many other hard problems, but for induced isomorphisms we
uncovered several interesting phenomena: there are hard instances far from a phase transition,
constrainedness predicts this, and existing general techniques for designing heuristics do not work
in certain portions of the parameter space.

It is worth noting that this technique does \emph{not} give a way of generating hard instances for
graph isomorphism problems: the pattern graph must be substantially smaller than the target graph
for independent pairs of randomly generated instances to give interesting behaviour.

?? This is not purely of theoretical interest. Stuff about graph databases.

\section*{Acknowledgements}

The authors wish to thank Kitty Meeks and Craig Reilly for their comments.

\bibliography{paper}
\bibliographystyle{theapa}

\clearpage
\appendix

\section{Reproducing These Experiments}

\end{document}
